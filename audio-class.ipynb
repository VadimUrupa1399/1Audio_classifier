{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-18T17:45:08.750878Z","iopub.execute_input":"2022-04-18T17:45:08.751419Z","iopub.status.idle":"2022-04-18T17:45:08.904042Z","shell.execute_reply.started":"2022-04-18T17:45:08.751296Z","shell.execute_reply":"2022-04-18T17:45:08.901443Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os \nimport math\nimport random\nimport zipfile\nimport requests\nimport warnings\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_io as tfio\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom IPython.display import Audio\nfrom IPython.core.display import display\nfrom matplotlib import pyplot as plt\nfrom pydub import AudioSegment, effects\nfrom pydub.generators import WhiteNoise\nfrom multiprocessing.pool import ThreadPool","metadata":{"execution":{"iopub.status.busy":"2022-04-18T17:45:59.794549Z","iopub.execute_input":"2022-04-18T17:45:59.795416Z","iopub.status.idle":"2022-04-18T17:46:08.428590Z","shell.execute_reply.started":"2022-04-18T17:45:59.795372Z","shell.execute_reply":"2022-04-18T17:46:08.427754Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Listen to Parsed Samples**","metadata":{}},{"cell_type":"code","source":"capuchin_files = os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\")\nnot_capuchin_files = os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Not_Capuchinbird_Clips\")\nCapuchin_File = \"XC3776-6.wav\" #random.choice(capuchin_files)\nNot_Capuchin_File = random.choice(not_capuchin_files)\nprint(f\"Displaying {Capuchin_File} which is an example of a Parsed Capuchinbird Call:\")\ndisplay(Audio(os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\",Capuchin_File)))\nprint(f\"Displaying {Not_Capuchin_File} which is an example of a Parsed Other Noise:\")\ndisplay(Audio(os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Not_Capuchinbird_Clips\",Not_Capuchin_File)))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T17:49:31.033890Z","iopub.execute_input":"2022-04-18T17:49:31.034675Z","iopub.status.idle":"2022-04-18T17:49:31.112176Z","shell.execute_reply.started":"2022-04-18T17:49:31.034627Z","shell.execute_reply":"2022-04-18T17:49:31.111571Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Data Augmentation**\n> Similar to Image Recognition in order to produce a robust model I want to Augment training set with transformed samples. Here I provide a few examples that can be used to transform audio clips.","metadata":{}},{"cell_type":"code","source":"def Add_White_Noise(sound, decibels = 50):\n    \"\"\"\n    Add White Noise to an Audio Clip and return the new clip\n    Note: sound should be an AudioSegment\n    \"\"\"\n    noise = WhiteNoise().to_audio_segment(duration=len(sound))-decibels\n    combined = sound.overlay(noise)\n    return combined\ndef Normalize_Volume(sound):\n    \"\"\"\n    Normalize the Volume of a Clip and return the new clip\n    Note :sound should be an AudioSegment\n    \"\"\"\n    normalized_sound = effects.normalize(sound) \n    return normalized_sound\ndef Filter_Out_High_Frequency(sound,cutoff = 8e3):\n    \"\"\"\n    Filter out High Frequencies in a Clip and return the new clip\n    Note: sound should be an AudioSegment and cutoff is in Hz (default is 8kHz)\n    \"\"\"\n    filtered_sound = effects.low_pass_filter(sound,cutoff) \n    return filtered_sound\ndef Filter_Out_Low_Frequency(sound,cutoff = 8e3):\n    \"\"\"\n    Filter out High Frequencies in a Clip and return the new clip\n    Note: sound should be an AudioSegment and cutoff is in Hz (default is 8kHz)\n    \"\"\"\n    filtered_sound = effects.high_pass_filter(sound,cutoff) \n    return filtered_sound","metadata":{"execution":{"iopub.status.busy":"2022-04-18T17:56:28.837639Z","iopub.execute_input":"2022-04-18T17:56:28.837921Z","iopub.status.idle":"2022-04-18T17:56:28.844825Z","shell.execute_reply.started":"2022-04-18T17:56:28.837891Z","shell.execute_reply":"2022-04-18T17:56:28.844023Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"***Model Training (Spectrograph and CNN)***","metadata":{}},{"cell_type":"code","source":"def decode_audio(audio_binary):\n    # Decode WAV-encoded audio files to `float32` tensors, normalized\n    # to the [-1.0, 1.0] range. Return `float32` audio and a sample rate.\n    audio, _ = tf.audio.decode_wav(contents=audio_binary,desired_channels=1,)\n    # Since all the data is single channel (mono), drop the `channels`\n    # axis from the array.\n    return tf.squeeze(audio, axis=-1)\ndef get_label(file_path):\n    parts = tf.strings.split(\n        input=file_path,\n        sep=os.path.sep)\n    # Note: You'll use indexing here instead of tuple unpacking to enable this\n    # to work in a TensorFlow graph.\n    return parts[-2]\ndef get_waveform_and_label(file_path):\n    label = get_label(file_path)\n    audio_binary = tf.io.read_file(file_path)\n    waveform = decode_audio(audio_binary)\n    return waveform, label\ndef get_spectrogram(waveform):\n    # Zero-padding for an audio waveform with less than 16,000 samples.\n    input_len = 16000\n    waveform = waveform[:input_len]\n    zero_padding = tf.zeros(\n    [16000] - tf.shape(waveform),\n    dtype=tf.float32)\n    # Cast the waveform tensors' dtype to float32.\n    waveform = tf.cast(waveform, dtype=tf.float32)\n    # Concatenate the waveform with `zero_padding`, which ensures all audio\n    # clips are of the same length.\n    equal_length = tf.concat([waveform, zero_padding], 0)\n    # Convert the waveform to a spectrogram via a STFT.\n    spectrogram = tf.signal.stft(\n    equal_length, frame_length=255, frame_step=128)\n    # Obtain the magnitude of the STFT.\n    spectrogram = tf.abs(spectrogram)\n    # Add a `channels` dimension, so that the spectrogram can be used\n    # as image-like input data with convolution layers (which expect\n    # shape (`batch_size`, `height`, `width`, `channels`).\n    spectrogram = spectrogram[..., tf.newaxis]\n    return spectrogram\ndef plot_spectrogram(spectrogram, ax):\n    if len(spectrogram.shape) > 2:\n        assert len(spectrogram.shape) == 3\n        spectrogram = np.squeeze(spectrogram, axis=-1)\n    # Convert the frequencies to log scale and transpose, so that the time is\n    # represented on the x-axis (columns).\n    # Add an epsilon to avoid taking a log of zero.\n    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n    height = log_spec.shape[0]\n    width = log_spec.shape[1]\n    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n    Y = range(height)\n    ax.pcolormesh(X, Y, log_spec)\ndef get_spectrogram_and_label_id(audio, label):\n    spectrogram = get_spectrogram(audio)\n    label_id = tf.argmax(label == commands)\n    return spectrogram, label_id\ndef preprocess_dataset(files):\n    files_ds = tf.data.Dataset.from_tensor_slices(files)\n    output_ds = files_ds.map(\n        map_func=get_waveform_and_label,\n        num_parallel_calls=AUTOTUNE)\n    output_ds = output_ds.map(\n        map_func=get_spectrogram_and_label_id,\n        num_parallel_calls=AUTOTUNE)\n    return output_ds","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:04:52.171979Z","iopub.execute_input":"2022-04-18T18:04:52.172258Z","iopub.status.idle":"2022-04-18T18:04:52.184932Z","shell.execute_reply.started":"2022-04-18T18:04:52.172230Z","shell.execute_reply":"2022-04-18T18:04:52.184114Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Build Dataset**","metadata":{}},{"cell_type":"code","source":"# Set the seed value for experiment reproducibility.\nseed = 1842\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n# Turn off warnings for cleaner looking notebook\nwarnings.simplefilter('ignore')\n\ndata_dir = \"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing\"\ncommands = [\"Parsed_Capuchinbird_Clips\",\"Parsed_Not_Capuchinbird_Clips\"]\nfilenames_Capuchinbird = tf.io.gfile.glob(str(data_dir) + '/Parsed_Capuchinbird_Clips/*')\nfilenames_Not_Capuchinbird = tf.io.gfile.glob(str(data_dir) + '/Parsed_Not_Capuchinbird_Clips/*')\nfilenames =tf.concat([filenames_Capuchinbird, filenames_Not_Capuchinbird], 0)\nfilenames = tf.random.shuffle(filenames)\nnum_samples = len(filenames)\nprint('Number of total examples:', num_samples)\n\ntrain_split = int(.8*num_samples)\nval_split = int(.1*num_samples)\ntest_split = num_samples - train_split - val_split\ntrain_files = filenames[:train_split]\nval_files = filenames[train_split: train_split + val_split]\ntest_files = filenames[-1*test_split:]\n\nprint('Training set size', len(train_files))\nprint('Validation set size', len(val_files))\nprint('Test set size', len(test_files))\n\nAUTOTUNE = tf.data.AUTOTUNE\n\nfiles_ds = tf.data.Dataset.from_tensor_slices(train_files)\n\nwaveform_ds = files_ds.map(\n    map_func=get_waveform_and_label,\n    num_parallel_calls=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:06:14.913608Z","iopub.execute_input":"2022-04-18T18:06:14.913885Z","iopub.status.idle":"2022-04-18T18:06:15.319245Z","shell.execute_reply.started":"2022-04-18T18:06:14.913856Z","shell.execute_reply":"2022-04-18T18:06:15.318378Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Visualize Dataset**","metadata":{}},{"cell_type":"code","source":"rows = 3\ncols = 3\nn = rows * cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 12))\nfor i, (audio, label) in enumerate(waveform_ds.take(n)):\n    r = i // cols\n    c = i % cols\n    ax = axes[r][c]\n    ax.plot(audio.numpy())\n    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n    label = label.numpy().decode('utf-8')\n    ax.set_title(label)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:09:29.782059Z","iopub.execute_input":"2022-04-18T18:09:29.782359Z","iopub.status.idle":"2022-04-18T18:09:31.213684Z","shell.execute_reply.started":"2022-04-18T18:09:29.782327Z","shell.execute_reply":"2022-04-18T18:09:31.212813Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"for waveform, label in waveform_ds.take(1):\n    label = label.numpy().decode('utf-8')\n    spectrogram = get_spectrogram(waveform)\n\nprint('Label:', label)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\ndisplay(Audio(waveform, rate=16000))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:10:28.338213Z","iopub.execute_input":"2022-04-18T18:10:28.339273Z","iopub.status.idle":"2022-04-18T18:10:28.478332Z","shell.execute_reply.started":"2022-04-18T18:10:28.339221Z","shell.execute_reply":"2022-04-18T18:10:28.477557Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(2, figsize=(12, 8))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title('Waveform')\naxes[0].set_xlim([0, 16000])\n\nplot_spectrogram(spectrogram.numpy(), axes[1])\naxes[1].set_title('Spectrogram')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:11:09.080940Z","iopub.execute_input":"2022-04-18T18:11:09.081303Z","iopub.status.idle":"2022-04-18T18:11:09.427741Z","shell.execute_reply.started":"2022-04-18T18:11:09.081259Z","shell.execute_reply":"2022-04-18T18:11:09.426762Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"spectrogram_ds = waveform_ds.map(\n  map_func=get_spectrogram_and_label_id,\n  num_parallel_calls=AUTOTUNE)\n\nrows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n\nfor i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n    r = i // cols\n    c = i % cols\n    ax = axes[r][c]\n    plot_spectrogram(spectrogram.numpy(), ax)\n    ax.set_title(commands[label_id.numpy()])\n    ax.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:12:58.026553Z","iopub.execute_input":"2022-04-18T18:12:58.026848Z","iopub.status.idle":"2022-04-18T18:12:59.166707Z","shell.execute_reply.started":"2022-04-18T18:12:58.026817Z","shell.execute_reply":"2022-04-18T18:12:59.165812Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**BUILD AND TRIN MODEL**","metadata":{}},{"cell_type":"code","source":"train_ds = spectrogram_ds\nval_ds = preprocess_dataset(val_files)\ntest_ds = preprocess_dataset(test_files)\n\nbatch_size = 64\ntrain_ds = train_ds.batch(batch_size)\nval_ds = val_ds.batch(batch_size)\n\ntrain_ds = train_ds.cache().prefetch(AUTOTUNE)\nval_ds = val_ds.cache().prefetch(AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:14:39.494592Z","iopub.execute_input":"2022-04-18T18:14:39.494880Z","iopub.status.idle":"2022-04-18T18:14:39.733256Z","shell.execute_reply.started":"2022-04-18T18:14:39.494849Z","shell.execute_reply":"2022-04-18T18:14:39.732454Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for spectrogram, _ in spectrogram_ds.take(1):\n    input_shape = spectrogram.shape\nprint('Input shape:', input_shape)\nnum_labels = len(commands)\n\n# Instantiate the `tf.keras.layers.Normalization` layer.\nnorm_layer = layers.Normalization()\n# Fit the state of the layer to the spectrograms\n# with `Normalization.adapt`.\nnorm_layer.adapt(data=spectrogram_ds.map(map_func=lambda spec, label: spec))\n\ncnn_model = models.Sequential([\n    layers.Input(shape=input_shape),\n    # Downsample the input.\n    layers.Resizing(32, 32),\n    # Normalize.\n    norm_layer,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n\ncnn_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:19:51.692270Z","iopub.execute_input":"2022-04-18T18:19:51.692578Z","iopub.status.idle":"2022-04-18T18:19:56.001609Z","shell.execute_reply.started":"2022-04-18T18:19:51.692542Z","shell.execute_reply":"2022-04-18T18:19:56.000743Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"cnn_model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)\nEPOCHS = 20\nhistory = cnn_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, \n                                               patience=5,\n                                               restore_best_weights=True\n                                              ),\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:20:40.905452Z","iopub.execute_input":"2022-04-18T18:20:40.905785Z","iopub.status.idle":"2022-04-18T18:21:06.907546Z","shell.execute_reply.started":"2022-04-18T18:20:40.905750Z","shell.execute_reply":"2022-04-18T18:21:06.906803Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"****Investigate Model Performance****","metadata":{}},{"cell_type":"code","source":"metrics = history.history\nplt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:24:06.077445Z","iopub.execute_input":"2022-04-18T18:24:06.078401Z","iopub.status.idle":"2022-04-18T18:24:06.219443Z","shell.execute_reply.started":"2022-04-18T18:24:06.078358Z","shell.execute_reply":"2022-04-18T18:24:06.218873Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test_audio = []\ntest_labels = []\n\nfor audio, label in test_ds:\n    test_audio.append(audio.numpy())\n    test_labels.append(label.numpy())\n\ntest_audio = np.array(test_audio)\ntest_labels = np.array(test_labels)\n\ny_pred = np.argmax(cnn_model.predict(test_audio), axis=1)\ny_true = test_labels\n\ntest_acc = sum(y_pred == y_true) / len(y_true)\nprint(f'Test set accuracy: {test_acc:.0%}')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:24:41.168237Z","iopub.execute_input":"2022-04-18T18:24:41.168646Z","iopub.status.idle":"2022-04-18T18:24:42.030000Z","shell.execute_reply.started":"2022-04-18T18:24:41.168600Z","shell.execute_reply":"2022-04-18T18:24:42.028950Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx,\n            xticklabels=commands,\n            yticklabels=commands,\n            annot=True, fmt='g')\nplt.xlabel('Prediction')\nplt.ylabel('Label')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:26:36.571674Z","iopub.execute_input":"2022-04-18T18:26:36.571940Z","iopub.status.idle":"2022-04-18T18:26:36.811517Z","shell.execute_reply.started":"2022-04-18T18:26:36.571911Z","shell.execute_reply":"2022-04-18T18:26:36.810552Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"capuchin_files = os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\")\nsample_file = random.choice(capuchin_files)\nprint(sample_file)\nsample_ds = preprocess_dataset([os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\",sample_file)])\n\nfor spectrogram, label in sample_ds.batch(1):\n    prediction = cnn_model(spectrogram)\n    plt.bar(commands, tf.nn.softmax(prediction[0]))\n    plt.title(f'Predictions for \"{commands[label[0]]}\"')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:28:19.888226Z","iopub.execute_input":"2022-04-18T18:28:19.888534Z","iopub.status.idle":"2022-04-18T18:28:20.346052Z","shell.execute_reply.started":"2022-04-18T18:28:19.888473Z","shell.execute_reply":"2022-04-18T18:28:20.345209Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Model Training (Transfer Learning)**","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef load_wav_16k_mono(filename):\n    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n    file_contents = tf.io.read_file(filename)\n    wav, sample_rate = tf.audio.decode_wav(\n          file_contents,\n          desired_channels=1)\n    wav = tf.squeeze(wav, axis=-1)\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n    return wav\ndef load_wav_for_map(filename, label):\n    return load_wav_16k_mono(filename), label\ndef extract_embedding(wav_data, label):\n    \"\"\" run YAMNet to extract embedding from the wav data \"\"\"\n    scores, embeddings, spectrogram = yamnet_model(wav_data)\n    num_embeddings = tf.shape(embeddings)[0]\n    return (embeddings,\n            tf.repeat(label, num_embeddings)\n           )\ndef get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n    \"\"\" Split Train, Test and Validation Datasets out of Dataframe \"\"\"\n    assert (train_split + test_split + val_split) == 1\n    \n    if shuffle:\n        # Specify seed to always have the same split distribution between runs\n        ds = ds.shuffle(shuffle_size, seed=1842)\n    \n    train_size = int(train_split * ds_size)\n    val_size = int(val_split * ds_size)\n    \n    train_ds = ds.take(train_size)    \n    val_ds = ds.skip(train_size).take(val_size)\n    test_ds = ds.skip(train_size).skip(val_size)\n    \n    return train_ds, val_ds, test_ds\n# Filter out Annoying Tensorflow Warnings\ntf.get_logger().setLevel('ERROR')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:30:55.026319Z","iopub.execute_input":"2022-04-18T18:30:55.026880Z","iopub.status.idle":"2022-04-18T18:30:55.037159Z","shell.execute_reply.started":"2022-04-18T18:30:55.026833Z","shell.execute_reply":"2022-04-18T18:30:55.036571Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Download YAMNet pretrained model**","metadata":{}},{"cell_type":"code","source":"yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\nyamnet_model = hub.load(yamnet_model_handle)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:31:21.959652Z","iopub.execute_input":"2022-04-18T18:31:21.960044Z","iopub.status.idle":"2022-04-18T18:31:29.729839Z","shell.execute_reply.started":"2022-04-18T18:31:21.960015Z","shell.execute_reply":"2022-04-18T18:31:29.728816Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Build Training Set**","metadata":{}},{"cell_type":"code","source":"capuchin_files = os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\")\ncapuchin = []\nfor file in capuchin_files:\n    if file.endswith('.wav'):\n        capuchin.append(os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\",file))\nnot_capuchin_files = os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Not_Capuchinbird_Clips\")\nnot_capuchin = []\nfor file in not_capuchin_files:\n    if file.endswith('.wav'):\n        not_capuchin.append(os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Not_Capuchinbird_Clips\",file))\n\ncapuchin_pd = pd.DataFrame({\"filename\":capuchin,\"target\":1})\nnot_capuchin_pd = pd.DataFrame({\"filename\":not_capuchin,\"target\":0})\ndataset_pd = pd.concat([capuchin_pd,not_capuchin_pd],axis=0,ignore_index=True)\ndataset_pd\n\nmain_ds = tf.data.Dataset.from_tensor_slices((dataset_pd[\"filename\"], dataset_pd[\"target\"]))\nmain_ds = main_ds.map(load_wav_for_map)\nmain_ds.element_spec\n\n# extract embedding\nmain_ds = main_ds.map(extract_embedding).unbatch()\nmain_ds.element_spec\n\ntrain_ds, val_ds, test_ds = get_dataset_partitions_tf(main_ds,len(dataset_pd))\ntrain_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\ntest_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:33:17.613419Z","iopub.execute_input":"2022-04-18T18:33:17.614165Z","iopub.status.idle":"2022-04-18T18:33:18.491757Z","shell.execute_reply.started":"2022-04-18T18:33:17.614125Z","shell.execute_reply":"2022-04-18T18:33:18.491006Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**Build Model**","metadata":{}},{"cell_type":"code","source":"yamnet_transfer_learning_model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n                          name='input_embedding'),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(2)\n], name='yamnet_transfer_learning_model')\n\nyamnet_transfer_learning_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:33:42.889235Z","iopub.execute_input":"2022-04-18T18:33:42.890480Z","iopub.status.idle":"2022-04-18T18:33:42.929683Z","shell.execute_reply.started":"2022-04-18T18:33:42.890411Z","shell.execute_reply":"2022-04-18T18:33:42.928782Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"yamnet_transfer_learning_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                 optimizer=\"adam\",\n                 metrics=['accuracy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n                                            patience=3,\n                                            restore_best_weights=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:34:43.146645Z","iopub.execute_input":"2022-04-18T18:34:43.146954Z","iopub.status.idle":"2022-04-18T18:34:43.158301Z","shell.execute_reply.started":"2022-04-18T18:34:43.146921Z","shell.execute_reply":"2022-04-18T18:34:43.157529Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{}},{"cell_type":"code","source":"history = yamnet_transfer_learning_model.fit(train_ds,\n                       epochs=20,\n                       validation_data=val_ds,\n                       callbacks=callback)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:35:16.310606Z","iopub.execute_input":"2022-04-18T18:35:16.311529Z","iopub.status.idle":"2022-04-18T18:36:49.279412Z","shell.execute_reply.started":"2022-04-18T18:35:16.311470Z","shell.execute_reply":"2022-04-18T18:36:49.278495Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Investigate model perfomance**","metadata":{}},{"cell_type":"code","source":"loss, accuracy = yamnet_transfer_learning_model.evaluate(test_ds)\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)\n\nhistory = yamnet_transfer_learning_model.fit(train_ds,\n                       epochs=20,\n                       validation_data=val_ds,\n                       callbacks=callback)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:43:04.218703Z","iopub.execute_input":"2022-04-18T18:43:04.219238Z","iopub.status.idle":"2022-04-18T18:43:55.345489Z","shell.execute_reply.started":"2022-04-18T18:43:04.219195Z","shell.execute_reply":"2022-04-18T18:43:55.344543Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Use Model Outputs to Count Capuchinbird Calls","metadata":{}},{"cell_type":"markdown","source":"**Simple Test Case Generator**","metadata":{}},{"cell_type":"code","source":"def locations_to_approx_seconds(location):\n    return str(location*3)\ndef locations_to_approx_result_row(location):\n    return str(int(location*6.5))\ndef make_tests(capuchin_path,not_capuchin_path,capuchin_count):\n    seconds = 1000\n    capuchin_sound = AudioSegment.from_wav(capuchin_path)\n    not_capuchin_sound = AudioSegment.from_wav(not_capuchin_path)\n    total_clips = 60\n    locations = random.sample(range(1, total_clips), capuchin_count)\n    locations.sort()\n    clip_positions = \",\".join(map(str,locations))\n    approx_locations_sec = \",\".join(map(locations_to_approx_seconds,locations))\n    approx_locations_result_row = \",\".join(map(locations_to_approx_result_row,locations))\n    print(f\"Capuchin Calls are Located at [{clip_positions}] positions in the clip\")\n    print(f\"Capuchin Calls are Located around [{approx_locations_sec}] seconds in the clip\")\n    print(f\"Capuchin Calls are Located around [{approx_locations_result_row}] in the result rows\")\n    clips = []\n    for i in range(total_clips):\n        if i in locations:\n            clips.append(capuchin_sound)\n        else:\n            test = random.sample([0,1],1)\n            if test == 0:\n                clips.append(WhiteNoise().to_audio_segment(duration=len(3*1000)))\n            else:\n                clips.append(not_capuchin_sound)\n    final_clip = clips[0]\n    for i in range(1,len(clips)):\n        final_clip = final_clip + clips[i]\n    output_file = \"test.wav\"\n    final_clip.export(output_file, format=\"wav\")\n    return output_file","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:49:38.746703Z","iopub.execute_input":"2022-04-18T18:49:38.747021Z","iopub.status.idle":"2022-04-18T18:49:38.757418Z","shell.execute_reply.started":"2022-04-18T18:49:38.746986Z","shell.execute_reply":"2022-04-18T18:49:38.756459Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Generate Test Case**","metadata":{}},{"cell_type":"code","source":"capuchin_files = os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\")\nnot_capuchin_files = os.listdir(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Not_Capuchinbird_Clips\")\nCapuchin_File = random.choice(capuchin_files)\nNot_Capuchin_File = random.choice(not_capuchin_files)\nNum_Capuchin_Calls = 5\nprint(f\"Using {Capuchin_File} and {Not_Capuchin_File} to generate {Num_Capuchin_Calls} Capuchinbird Calls\")\n\nnot_capuchin_path = os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Not_Capuchinbird_Clips\",Not_Capuchin_File)\ncapuchin_path = os.path.join(\"/kaggle/input/z-by-hp-unlocked-challenge-3-signal-processing/Parsed_Capuchinbird_Clips\",Capuchin_File)\ntest_file_name = make_tests(capuchin_path,not_capuchin_path,Num_Capuchin_Calls)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:52:08.933041Z","iopub.execute_input":"2022-04-18T18:52:08.933352Z","iopub.status.idle":"2022-04-18T18:52:09.742180Z","shell.execute_reply.started":"2022-04-18T18:52:08.933315Z","shell.execute_reply":"2022-04-18T18:52:09.741326Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Generate Model Results**","metadata":{}},{"cell_type":"code","source":"testing_wav_data = load_wav_16k_mono(test_file_name)\nscores, embeddings, spectrogram = yamnet_model(testing_wav_data)\nresult = yamnet_transfer_learning_model(embeddings).numpy()\nresult","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:53:09.573329Z","iopub.execute_input":"2022-04-18T18:53:09.573608Z","iopub.status.idle":"2022-04-18T18:53:13.480845Z","shell.execute_reply.started":"2022-04-18T18:53:09.573577Z","shell.execute_reply":"2022-04-18T18:53:13.480040Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"**Simple Capuchinbird Call Counter**\n> All Positive Model Scores for the Positive Class (Capuchinbird) that occur in a row as a single call. It is easy to see cases where this will fail to properly count the calls so building a more complex Call Counter will be up to you and an important piece of your solution","metadata":{}},{"cell_type":"code","source":"count = 0\nprevious_pos = 0\ncapuchin_count = 0\nprint(\"Embeddings with Positive Val for Capuchinbird Call:\")\nfor row in result:\n    if row[1]>0:\n        if count - previous_pos > 1:\n            capuchin_count += 1\n        previous_pos = count\n        value = '%.2f' % round(row[1],2)\n        if count <100:\n            print(f\"  row:  {count} value: {value}\")\n        else:\n            print(f\"  row: {count} value: {value}\")\n    count += 1\nprint(f\"Found {capuchin_count} of {Num_Capuchin_Calls} Capuchin Calls!\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T18:54:22.669472Z","iopub.execute_input":"2022-04-18T18:54:22.669777Z","iopub.status.idle":"2022-04-18T18:54:22.680808Z","shell.execute_reply.started":"2022-04-18T18:54:22.669741Z","shell.execute_reply":"2022-04-18T18:54:22.679717Z"},"trusted":true},"execution_count":29,"outputs":[]}]}